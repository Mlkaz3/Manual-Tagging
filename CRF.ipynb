{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c49e4eb-f131-4979-b735-00d8711a8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time\n",
    "\n",
    "import sklearn\n",
    "import sklearn_crfsuite\n",
    "import scipy.stats\n",
    "import math, string, re\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn_crfsuite import CRF\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag.util import untag\n",
    "\n",
    "import ast\n",
    "from ast import literal_eval\n",
    "\n",
    "import jieba \n",
    "from hanziconv import HanziConv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef303f6-85e9-4adf-936f-4188c8ef2e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/pc8/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words, brown, treebank\n",
    "nltk.download('treebank')\n",
    "\n",
    "nltk_treebank = (treebank.tagged_sents())\n",
    "\n",
    "def penn2ud(text):\n",
    "    if text in ['#', '$','SYM']:\n",
    "        return 'E_SYM'\n",
    "    elif text in [r\"''\", ',', '-LRB-', '-RRB-', '.', ':', 'HYPH', '``']:\n",
    "        return 'E_PUNCT'\n",
    "    elif text in ['AFX', 'JJ', 'JJR', 'JJS']:\n",
    "        return 'E_ADJ'\n",
    "    elif text in ['RB', 'RBR', 'RBS']:\n",
    "        return 'E_ADV'\n",
    "    elif text == 'CC':\n",
    "        return 'E_CCONJ'\n",
    "    elif text in ['DT', 'PDT', 'PRP$', 'WDT', 'WP$']:\n",
    "        return 'E_DET'\n",
    "    elif text == 'CD':\n",
    "        return 'E_NUM'\n",
    "    elif text in ['EX','PRP', 'WP']:\n",
    "        return 'E_PRON'\n",
    "    elif text in ['FW', 'LS', 'NIL']:\n",
    "        return 'E_X'\n",
    "    elif text in ['IN', 'RP']:\n",
    "        return 'E_ADP'\n",
    "    elif text in ['MD','VB', 'VBD', 'VBG', 'VBN' ,'VBP' ,'VBZ']:\n",
    "        return 'E_VERB'\n",
    "    elif text in ['NN', 'NNS']:\n",
    "        return 'E_NOUN'\n",
    "    elif text in ['NNP', 'NNPS']:\n",
    "        return 'E_PROPN'\n",
    "    elif text in ['POS', 'TO']:\n",
    "        return 'E_PART'\n",
    "    elif text == 'UH':\n",
    "        return 'E_INTJ'\n",
    "    else:\n",
    "        return 'E_X'\n",
    "\n",
    "def replace_E_(text):\n",
    "    return re.sub(r\"E_\", \"\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "136d6767-f6ab-4267-92a5-6c2340606774",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f75fa5-76f6-4a88-829e-2d1a95c6e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_treebank = []\n",
    "for sentence in nltk_treebank:\n",
    "    sentence_list = [] \n",
    "    for word in sentence:\n",
    "        word_list = list(word)\n",
    "        tag = penn2ud(word_list[1])\n",
    "        tag = replace_E_(tag)\n",
    "        word_list[1] = tag\n",
    "        word_tuple = tuple(word_list)\n",
    "        sentence_list.append(word_tuple)\n",
    "    full_treebank.append(sentence_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5790e17a-ea34-48b0-afb3-6cd0073b05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Corpus/English/treebank_Mapped.txt', 'w') as f:\n",
    "    for line in full_treebank:\n",
    "        f.write(str(line))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a9ad53-2d9e-4fd2-b5bf-caf34873034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'PROPN'), ('Vinken', 'PROPN'), (',', 'PUNCT'), ('61', 'NUM'), ('years', 'NOUN'), ('old', 'ADJ'), (',', 'PUNCT'), ('will', 'VERB'), ('join', 'VERB'), ('the', 'DET'), ('board', 'NOUN'), ('as', 'ADP'), ('a', 'DET'), ('nonexecutive', 'ADJ'), ('director', 'NOUN'), ('Nov.', 'PROPN'), ('29', 'NUM'), ('.', 'PUNCT')], [('Mr.', 'PROPN'), ('Vinken', 'PROPN'), ('is', 'VERB'), ('chairman', 'NOUN'), ('of', 'ADP'), ('Elsevier', 'PROPN'), ('N.V.', 'PROPN'), (',', 'PUNCT'), ('the', 'DET'), ('Dutch', 'PROPN'), ('publishing', 'VERB'), ('group', 'NOUN'), ('.', 'PUNCT')]]\n"
     ]
    }
   ],
   "source": [
    "print(full_treebank[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94c9fcbc-79d8-48d4-8d5d-4e7db159efb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open(\"../Corpus/English/Brown_Mapped/part-00000\", \"r\")\n",
    "\n",
    "brown_word_list = []\n",
    "for l in f:\n",
    "    brown_word_list.append(l)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d22c993c-39e3-445a-a8ed-1c7c9b051546",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Alldata = pd.read_csv('../Data/Sample_Data/Sample_All.csv')\n",
    "df_training = pd.read_csv(r'manuallyTagging.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b47d30-4498-4c1f-8e43-91d4f6cc2f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = pd.merge(df_Alldata, df_training, how='inner',\n",
    "                  left_on=['Sentence'], right_on=['f'])\n",
    "\n",
    "# drop the indices from USERS\n",
    "df_Alldata = df_Alldata.drop(duplicates.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40f5ad0d-c107-4571-9592-979c4da3dca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178300"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Alldata = df_Alldata.drop(['Unnamed: 0', 'Key', 'token_words'], axis = 1)\n",
    "len(df_Alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76a46cd7-7148-4d60-a388-493ff945fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(r'manuallyTagging.csv')\n",
    "# df = df.dropna()\n",
    "\n",
    "df = pd.read_csv(r'testing posTAG.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "def remove_newline(text):\n",
    "    return re.sub(r'\\n', ' ', str(text))\n",
    "\n",
    "def find_astrophe(text):\n",
    "    return bool(re.findall(r'\\'', text))\n",
    "\n",
    "def convert_string2_list(text):\n",
    "\n",
    "    text = ast.literal_eval(str(text))\n",
    "    return text\n",
    "\n",
    "def unicode_problem(text):\n",
    "    return re.sub(r'[\\u0080]','',text).strip()\n",
    "\n",
    "def unprinable(text):\n",
    "     return re.sub(r'[\\u200B]','',text).strip()\n",
    "    \n",
    "def replace_apostrophes(text):\n",
    "    return re.sub('&#39;|’|´|‘', \"'\", (str(text)))\n",
    "\n",
    "templist = [] \n",
    "for i in df[\"Pos Tag\"]:\n",
    "    i = remove_newline(i)\n",
    "    i = unicode_problem(i)\n",
    "    i = unprinable(i)\n",
    "    i = replace_apostrophes(i)\n",
    "    i = i.strip()\n",
    "    i = convert_string2_list(i)\n",
    "    templist.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "649e0a89-25ec-428a-a2d0-e1a829f4e9af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('In', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('webinar', 'NOUN'),\n",
       "  ('last', 'ADJ'),\n",
       "  ('week', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('Capital', 'PROPN'),\n",
       "  ('Economics', 'PROPN'),\n",
       "  (\"\\\\'\", 'PART'),\n",
       "  ('global', 'ADJ'),\n",
       "  ('economists', 'NOUN'),\n",
       "  ('lumped', 'VERB'),\n",
       "  ('together', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('Philippines', 'PROPN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('Thailand', 'PROPN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('Mexico', 'PROPN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('Southern', 'PROPN'),\n",
       "  ('Europe', 'PROPN'),\n",
       "  ('among', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('economies', 'NOUN'),\n",
       "  ('which', 'DET'),\n",
       "  ('would', 'AUX'),\n",
       "  ('most', 'ADV'),\n",
       "  ('likely', 'ADJ'),\n",
       "  ('experience', 'NOUN'),\n",
       "  ('permanent', 'ADJ'),\n",
       "  ('loss', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('output', 'NOUN'),\n",
       "  ('from', 'ADP'),\n",
       "  ('their', 'PRON'),\n",
       "  ('respective', 'ADJ'),\n",
       "  ('domestic', 'ADJ'),\n",
       "  ('tourism', 'NOUN'),\n",
       "  ('sectors', 'NOUN'),\n",
       "  ('due', 'ADP'),\n",
       "  ('to', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('pandemic', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('I', 'PRON'),\n",
       "  ('think', 'VERB'),\n",
       "  ('that', 'DET'),\n",
       "  (\"\\\\'s\", 'PART'),\n",
       "  ('not', 'ADV'),\n",
       "  ('an', 'DET'),\n",
       "  ('official', 'ADJ'),\n",
       "  ('unbox', 'NOUN'),\n",
       "  ('therapy', 'NOUN'),\n",
       "  ('account', 'NOUN')],\n",
       " [('Goooood', 'X'), (',', 'PUNCT'), ('very', 'ADV'), ('soft', 'ADJ')],\n",
       " [('!', 'PUNCT'),\n",
       "  ('nice', 'ADJ'),\n",
       "  ('to', 'ADP'),\n",
       "  ('wear', 'VERB'),\n",
       "  ('Can', 'AUX'),\n",
       "  ('repeat', 'VERB'),\n",
       "  ('order', 'VERB'),\n",
       "  ('next', 'ADJ'),\n",
       "  ('time', 'NOUN')],\n",
       " [('item', 'NOUN'),\n",
       "  ('with', 'ADP'),\n",
       "  ('nice', 'ADJ'),\n",
       "  ('packing', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('quality', 'NOUN'),\n",
       "  ('product', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('value', 'NOUN'),\n",
       "  ('for', 'ADP'),\n",
       "  ('money', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('Excellent', 'ADJ'),\n",
       "  ('service', 'NOUN'),\n",
       "  ('by', 'ADP'),\n",
       "  ('seller', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('I', 'PRON'),\n",
       "  (\"can\\\\'t\", 'AUX'),\n",
       "  ('search', 'VERB'),\n",
       "  ('anymore', 'ADV'),\n",
       "  ('Jollibee', 'PROPN'),\n",
       "  ('store', 'NOUN'),\n",
       "  ('near', 'ADP'),\n",
       "  ('me', 'PRON'),\n",
       "  ('I', 'PRON'),\n",
       "  ('had', 'AUX'),\n",
       "  ('ordered', 'VERB'),\n",
       "  ('last', 'ADJ'),\n",
       "  ('week', 'NOUN'),\n",
       "  ('but', 'CCONJ'),\n",
       "  ('now', 'ADV'),\n",
       "  ('?', 'PUNCT'),\n",
       "  ('?', 'PUNCT'),\n",
       "  ('?', 'PUNCT')],\n",
       " [('Fast', 'ADJ'),\n",
       "  ('delivery', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('packing', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('value', 'NOUN'),\n",
       "  ('product', 'NOUN'),\n",
       "  ('.', 'PUNCT'),\n",
       "  ('Excellent', 'ADJ'),\n",
       "  ('service', 'NOUN')],\n",
       " [('Not', 'ADV'), ('try', 'VERB'), ('yet', 'ADV'), ('.', 'PUNCT')],\n",
       " [('But', 'CCONJ'),\n",
       "  ('looks', 'VERB'),\n",
       "  ('like', 'ADP'),\n",
       "  ('just', 'ADV'),\n",
       "  ('nice', 'ADJ'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Suitable', 'ADJ'),\n",
       "  ('with', 'ADP'),\n",
       "  ('my', 'PRON'),\n",
       "  ('skin', 'NOUN'),\n",
       "  ('.', 'PUNCT')]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templist[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d94d9ea1-ed39-45d3-ab70-54a0da9f427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6318"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_data = templist + full_treebank\n",
    "# nltk_data = templist\n",
    "len(nltk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2970a27e-6e3e-417a-b979-00a30c918aed",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('In', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('webinar', 'NOUN'),\n",
       "  ('last', 'ADJ'),\n",
       "  ('week', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('Capital', 'PROPN'),\n",
       "  ('Economics', 'PROPN'),\n",
       "  (\"\\\\'\", 'PART'),\n",
       "  ('global', 'ADJ'),\n",
       "  ('economists', 'NOUN'),\n",
       "  ('lumped', 'VERB'),\n",
       "  ('together', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('Philippines', 'PROPN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('Thailand', 'PROPN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('Mexico', 'PROPN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('Southern', 'PROPN'),\n",
       "  ('Europe', 'PROPN'),\n",
       "  ('among', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('economies', 'NOUN'),\n",
       "  ('which', 'DET'),\n",
       "  ('would', 'AUX'),\n",
       "  ('most', 'ADV'),\n",
       "  ('likely', 'ADJ'),\n",
       "  ('experience', 'NOUN'),\n",
       "  ('permanent', 'ADJ'),\n",
       "  ('loss', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('output', 'NOUN'),\n",
       "  ('from', 'ADP'),\n",
       "  ('their', 'PRON'),\n",
       "  ('respective', 'ADJ'),\n",
       "  ('domestic', 'ADJ'),\n",
       "  ('tourism', 'NOUN'),\n",
       "  ('sectors', 'NOUN'),\n",
       "  ('due', 'ADP'),\n",
       "  ('to', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('pandemic', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('I', 'PRON'),\n",
       "  ('think', 'VERB'),\n",
       "  ('that', 'DET'),\n",
       "  (\"\\\\'s\", 'PART'),\n",
       "  ('not', 'ADV'),\n",
       "  ('an', 'DET'),\n",
       "  ('official', 'ADJ'),\n",
       "  ('unbox', 'NOUN'),\n",
       "  ('therapy', 'NOUN'),\n",
       "  ('account', 'NOUN')],\n",
       " [('Goooood', 'X'), (',', 'PUNCT'), ('very', 'ADV'), ('soft', 'ADJ')],\n",
       " [('!', 'PUNCT'),\n",
       "  ('nice', 'ADJ'),\n",
       "  ('to', 'ADP'),\n",
       "  ('wear', 'VERB'),\n",
       "  ('Can', 'AUX'),\n",
       "  ('repeat', 'VERB'),\n",
       "  ('order', 'VERB'),\n",
       "  ('next', 'ADJ'),\n",
       "  ('time', 'NOUN')],\n",
       " [('item', 'NOUN'),\n",
       "  ('with', 'ADP'),\n",
       "  ('nice', 'ADJ'),\n",
       "  ('packing', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('quality', 'NOUN'),\n",
       "  ('product', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('value', 'NOUN'),\n",
       "  ('for', 'ADP'),\n",
       "  ('money', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('Excellent', 'ADJ'),\n",
       "  ('service', 'NOUN'),\n",
       "  ('by', 'ADP'),\n",
       "  ('seller', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('I', 'PRON'),\n",
       "  (\"can\\\\'t\", 'AUX'),\n",
       "  ('search', 'VERB'),\n",
       "  ('anymore', 'ADV'),\n",
       "  ('Jollibee', 'PROPN'),\n",
       "  ('store', 'NOUN'),\n",
       "  ('near', 'ADP'),\n",
       "  ('me', 'PRON'),\n",
       "  ('I', 'PRON'),\n",
       "  ('had', 'AUX'),\n",
       "  ('ordered', 'VERB'),\n",
       "  ('last', 'ADJ'),\n",
       "  ('week', 'NOUN'),\n",
       "  ('but', 'CCONJ'),\n",
       "  ('now', 'ADV'),\n",
       "  ('?', 'PUNCT'),\n",
       "  ('?', 'PUNCT'),\n",
       "  ('?', 'PUNCT')],\n",
       " [('Fast', 'ADJ'),\n",
       "  ('delivery', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('packing', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('value', 'NOUN'),\n",
       "  ('product', 'NOUN'),\n",
       "  ('.', 'PUNCT'),\n",
       "  ('Excellent', 'ADJ'),\n",
       "  ('service', 'NOUN')],\n",
       " [('Not', 'ADV'), ('try', 'VERB'), ('yet', 'ADV'), ('.', 'PUNCT')],\n",
       " [('But', 'CCONJ'),\n",
       "  ('looks', 'VERB'),\n",
       "  ('like', 'ADP'),\n",
       "  ('just', 'ADV'),\n",
       "  ('nice', 'ADJ'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Suitable', 'ADJ'),\n",
       "  ('with', 'ADP'),\n",
       "  ('my', 'PRON'),\n",
       "  ('skin', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Suitable', 'ADJ'),\n",
       "  ('with', 'ADP'),\n",
       "  ('my', 'PRON'),\n",
       "  ('tone', 'NOUN'),\n",
       "  ('color', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('skin', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Packaging', 'NOUN'), ('was', 'VERB'), ('good', 'ADJ'), ('.', 'PUNCT')],\n",
       " [('The', 'DET'),\n",
       "  ('texture', 'NOUN'),\n",
       "  ('was', 'VERB'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('The', 'DET'),\n",
       "  ('smell', 'NOUN'),\n",
       "  ('was', 'VERB'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('either', 'ADP'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('I', 'PRON'),\n",
       "  ('buy', 'VERB'),\n",
       "  ('this', 'DET'),\n",
       "  ('to', 'AUX'),\n",
       "  ('used', 'VERB'),\n",
       "  ('it', 'PRON'),\n",
       "  ('while', 'ADP'),\n",
       "  ('i', 'PRON'),\n",
       "  ('was', 'AUX'),\n",
       "  ('travelling', 'VERB'),\n",
       "  ('to', 'AUX'),\n",
       "  ('make', 'VERB'),\n",
       "  ('it', 'PRON'),\n",
       "  ('easier', 'ADJ'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Hopefully', 'ADV'),\n",
       "  ('it', 'PRON'),\n",
       "  ('work', 'VERB'),\n",
       "  ('on', 'ADP'),\n",
       "  ('me', 'PRON'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Thank', 'VERB'), ('you', 'PRON')],\n",
       " [('Nice', 'ADJ'),\n",
       "  ('Fast', 'ADJ'),\n",
       "  ('delivery', 'NOUN'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('free', 'ADJ'),\n",
       "  ('gift', 'NOUN')],\n",
       " [('Thank', 'VERB'), ('you', 'PRON'), ('YEs', 'PROPN'), ('Team', 'NOUN')],\n",
       " [('WhatsApp', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('contact', 'VERB'),\n",
       "  ('Marques', 'PROPN'),\n",
       "  ('as', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('winner', 'NOUN'),\n",
       "  ('!', 'PUNCT'),\n",
       "  ('!', 'PUNCT'),\n",
       "  ('!', 'PUNCT'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('?', 'PUNCT'),\n",
       "  (',', 'PUNCT')],\n",
       " [('Best', 'ADJ'), ('product', 'NOUN'), ('..', 'PUNCT')],\n",
       " [('Hmmmmmmmmmmmmmmmmmmmmmmmmmmmmm', 'INTJ')],\n",
       " [('..', 'PUNCT'), ('tq', 'X'), ('courier', 'NOUN')],\n",
       " [('The', 'DET'),\n",
       "  ('Camera', 'NOUN'),\n",
       "  ('Works', 'VERB'),\n",
       "  ('well', 'ADV'),\n",
       "  ('for', 'ADP'),\n",
       "  ('me', 'PRON'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('but', 'CCONJ'),\n",
       "  ('I', 'PRON'),\n",
       "  ('clearly', 'ADV'),\n",
       "  ('understand', 'VERB'),\n",
       "  ('some', 'DET'),\n",
       "  ('have', 'VERB'),\n",
       "  ('trouble', 'NOUN'),\n",
       "  ('with', 'ADP'),\n",
       "  ('it', 'PRON'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('I', 'PRON'),\n",
       "  ('am', 'VERB'),\n",
       "  ('sorry', 'ADJ'),\n",
       "  ('about', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('experience', 'NOUN'),\n",
       "  ('that', 'SCONJ'),\n",
       "  ('they', 'PRON'),\n",
       "  ('are', 'AUX'),\n",
       "  ('facing', 'VERB'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Good', 'ADJ'),\n",
       "  ('quality', 'NOUN'),\n",
       "  ('And', 'CCONJ'),\n",
       "  ('selling', 'VERB'),\n",
       "  ('at', 'ADP'),\n",
       "  ('cheap', 'ADJ'),\n",
       "  ('price', 'NOUN')],\n",
       " [('if', 'ADP'),\n",
       "  ('any', 'DET'),\n",
       "  ('of', 'ADP'),\n",
       "  ('you', 'PRON'),\n",
       "  ('wants', 'VERB'),\n",
       "  ('to', 'AUX'),\n",
       "  ('buy', 'VERB'),\n",
       "  ('decorated', 'VERB'),\n",
       "  ('toploader', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('do', 'VERB'),\n",
       "  ('visit', 'VERB'),\n",
       "  ('my', 'PRON'),\n",
       "  ('shopee', 'NOUN'),\n",
       "  (':', 'PUNCT'),\n",
       "  ('uwustoreee', 'PROPN')],\n",
       " [('good', 'ADJ'),\n",
       "  ('product', 'NOUN'),\n",
       "  ('quality', 'NOUN'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('value', 'NOUN'),\n",
       "  ('for', 'ADP'),\n",
       "  ('money', 'NOUN'),\n",
       "  ('fast', 'ADJ'),\n",
       "  ('delivery', 'NOUN')],\n",
       " [('Good', 'ADJ'),\n",
       "  ('product', 'NOUN'),\n",
       "  ('quality', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('fast', 'ADJ'),\n",
       "  ('deliver', 'VERB'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('order', 'VERB'),\n",
       "  ('on', 'ADP'),\n",
       "  ('25.9', 'NUM'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('receive', 'VERB'),\n",
       "  ('on', 'ADP'),\n",
       "  ('27.9', 'NUM'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Received', 'VERB'),\n",
       "  ('in', 'ADP'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('condition', 'NOUN'),\n",
       "  ('!', 'PUNCT')],\n",
       " [('Fast', 'ADJ'), ('delivery', 'NOUN'), ('!', 'PUNCT')],\n",
       " [('Good', 'ADJ'), ('Quality', 'NOUN'), ('!', 'PUNCT')],\n",
       " [('Good', 'ADJ'), ('!', 'PUNCT')],\n",
       " [('Thankyou', 'X'),\n",
       "  ('seller', 'NOUN'),\n",
       "  ('!', 'PUNCT'),\n",
       "  ('!', 'PUNCT'),\n",
       "  ('!', 'PUNCT'),\n",
       "  ('!', 'PUNCT'),\n",
       "  ('!', 'PUNCT')],\n",
       " [('Received', 'VERB'),\n",
       "  ('in', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('condition', 'NOUN'),\n",
       "  ('.', 'DET')],\n",
       " [('Thankyou', 'X'),\n",
       "  ('for', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('free', 'ADJ'),\n",
       "  ('gift', 'NOUN')],\n",
       " [('H', 'NOUN'), (':', 'PUNCT'), ('162', 'NUM')],\n",
       " [('tq', 'X'), ('seller', 'NOUN')],\n",
       " [('I', 'PRON'),\n",
       "  (\"\\\\'ll\", 'AUX'),\n",
       "  ('wait', 'VERB'),\n",
       "  ('for', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('14', 'NUM'),\n",
       "  ('pro', 'NOUN'),\n",
       "  ('..', 'PUNCT')],\n",
       " [('I', 'PRON'),\n",
       "  (\"'ll\", 'AUX'),\n",
       "  ('add', 'VERB'),\n",
       "  ('to', 'ADP'),\n",
       "  ('my', 'PRON'),\n",
       "  ('android', 'NOUN'),\n",
       "  ('collection', 'NOUN'),\n",
       "  ('..', 'PUNCT')],\n",
       " [('Very', 'ADV'), ('good', 'ADJ')],\n",
       " [('Ship', 'VERB'),\n",
       "  ('out', 'ADV'),\n",
       "  ('fast', 'ADJ'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('received', 'VERB'),\n",
       "  ('safely', 'ADV'),\n",
       "  ('in', 'ADP'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('condition', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Some', 'DET'),\n",
       "  ('exhibition', 'NOUN'),\n",
       "  ('spaces', 'NOUN'),\n",
       "  ('were', 'VERB'),\n",
       "  ('even', 'ADV'),\n",
       "  ('moved', 'VERB'),\n",
       "  ('online', 'ADV'),\n",
       "  ('so', 'SCONJ'),\n",
       "  ('that', 'SCONJ'),\n",
       "  ('people', 'NOUN'),\n",
       "  ('outside', 'ADP'),\n",
       "  ('Hanoi', 'PROPN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('or', 'CCONJ'),\n",
       "  ('those', 'DET'),\n",
       "  ('afraid', 'ADJ'),\n",
       "  ('of', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('infection', 'NOUN'),\n",
       "  ('risk', 'NOUN'),\n",
       "  ('in', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('community', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('can', 'AUX'),\n",
       "  ('enjoy', 'VERB'),\n",
       "  ('from', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('comfort', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('their', 'PRON'),\n",
       "  ('homes', 'NOUN'),\n",
       "  ('.', 'PUNCT'),\n",
       "  ('Item', 'NOUN'),\n",
       "  ('received', 'VERB'),\n",
       "  ('in', 'ADP'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('condition', 'NOUN')],\n",
       " [('Item', 'NOUN'),\n",
       "  ('received', 'VERB'),\n",
       "  ('in', 'ADP'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('condition', 'NOUN')],\n",
       " [('The', 'DET'),\n",
       "  ('packaging', 'NOUN'),\n",
       "  ('was', 'VERB'),\n",
       "  ('nice', 'ADJ'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('the', 'DET'),\n",
       "  ('items', 'NOUN'),\n",
       "  ('was', 'VERB'),\n",
       "  ('in', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('quality', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Delivery', 'NOUN'),\n",
       "  ('was', 'VERB'),\n",
       "  ('quick', 'ADV'),\n",
       "  ('too', 'ADV'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Would', 'AUX'),\n",
       "  ('buy', 'VERB'),\n",
       "  ('from', 'ADP'),\n",
       "  ('again', 'ADV'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Thank', 'VERB'),\n",
       "  ('you', 'PRON'),\n",
       "  ('for', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('gift', 'NOUN'),\n",
       "  ('!', 'PUNCT')],\n",
       " [('Sorry', 'ADJ'),\n",
       "  ('for', 'ADP'),\n",
       "  ('unrelated', 'ADJ'),\n",
       "  ('pic', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Received', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('product', 'NOUN'),\n",
       "  ('in', 'ADP'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('condition', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Havent', 'AUX'), ('tried', 'VERB'), ('.', 'PUNCT')],\n",
       " [('Packaging', 'NOUN'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('delivery', 'NOUN'),\n",
       "  ('smooth', 'ADJ'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Thanks', 'NOUN'), ('.', 'PUNCT')],\n",
       " [('Nice', 'ADJ'),\n",
       "  ('packaging', 'NOUN'),\n",
       "  ('nice', 'ADJ'),\n",
       "  ('delivery', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Fast', 'ADJ'),\n",
       "  ('work', 'VERB'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('nice', 'ADJ'),\n",
       "  ('quality', 'NOUN'),\n",
       "  ('would', 'AUX'),\n",
       "  ('repeat', 'VERB'),\n",
       "  ('order', 'VERB'),\n",
       "  ('next', 'ADJ'),\n",
       "  ('time', 'NOUN')],\n",
       " [('very', 'ADV'),\n",
       "  ('fast', 'ADJ'),\n",
       "  ('shipping', 'NOUN'),\n",
       "  ('very', 'ADV'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('seller', 'NOUN')],\n",
       " [('Fast', 'ADJ'), ('delivery', 'NOUN'), ('.', 'PUNCT')],\n",
       " [('Good', 'ADJ'),\n",
       "  ('value', 'NOUN'),\n",
       "  ('for', 'ADP'),\n",
       "  ('money', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Received', 'VERB'),\n",
       "  ('in', 'ADP'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('condition', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('It', 'PRON'),\n",
       "  (\"'s\", 'PART'),\n",
       "  ('arrived', 'VERB'),\n",
       "  ('safely', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('packaging', 'NOUN'),\n",
       "  ('is', 'VERB'),\n",
       "  ('great', 'ADJ'),\n",
       "  ('so', 'ADV'),\n",
       "  ('thick', 'ADJ'),\n",
       "  ('bubble', 'NOUN'),\n",
       "  ('wrap', 'NOUN'),\n",
       "  ('thankyousomuch', 'NOUN')],\n",
       " [('This', 'DET'),\n",
       "  ('is', 'VERB'),\n",
       "  ('3rd', 'NUM'),\n",
       "  ('time', 'NOUN'),\n",
       "  ('I', 'PRON'),\n",
       "  ('repeat', 'VERB'),\n",
       "  ('order', 'VERB'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('good', 'ADJ')],\n",
       " [('Quality', 'NOUN'), (':', 'PUNCT'), ('Good', 'ADJ')],\n",
       " [('Quality', 'NOUN'), (':', 'PUNCT')],\n",
       " [('Cantiikkkkkk', 'X'),\n",
       "  ('its', 'PRON'),\n",
       "  ('very', 'ADV'),\n",
       "  ('pretty', 'ADJ'),\n",
       "  ('apparently', 'ADV'),\n",
       "  ('as', 'ADP'),\n",
       "  ('for', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('size', 'NOUN'),\n",
       "  ('its', 'PRON'),\n",
       "  ('satisfying', 'VERB'),\n",
       "  ('like', 'ADP'),\n",
       "  ('enuf', 'X'),\n",
       "  ('to', 'AUX'),\n",
       "  ('put', 'VERB'),\n",
       "  ('only', 'ADV'),\n",
       "  ('your', 'PRON'),\n",
       "  ('phone', 'NOUN')],\n",
       " [('Dah', 'ADV'), ('download', 'VERB'), ('App', 'NOUN'), ('FitPro', 'PROPN')],\n",
       " [('Aplikasinya', 'NOUN'),\n",
       "  ('makin', 'ADV'),\n",
       "  ('lama', 'ADJ'),\n",
       "  ('makin', 'ADV'),\n",
       "  ('payah', 'ADJ')],\n",
       " [('Received', 'VERB'), ('in', 'ADP'), ('good', 'ADJ'), ('condition', 'NOUN')],\n",
       " [('Its', 'PRON'),\n",
       "  ('quality', 'NOUN'),\n",
       "  ('is', 'VERB'),\n",
       "  ('quite', 'ADV'),\n",
       "  ('ok', 'ADJ'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('not', 'ADV'),\n",
       "  ('too', 'ADV'),\n",
       "  ('thin', 'ADJ'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('semitransparent', 'ADJ'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('I', 'PRON'),\n",
       "  (\"'ll\", 'AUX'),\n",
       "  ('use', 'VERB'),\n",
       "  ('it', 'PRON'),\n",
       "  ('to', 'AUX'),\n",
       "  ('store', 'VERB'),\n",
       "  ('my', 'PRON'),\n",
       "  ('not', 'ADV'),\n",
       "  ('usual', 'ADJ'),\n",
       "  ('wearing', 'VERB'),\n",
       "  ('clothes', 'NOUN'),\n",
       "  ('to', 'AUX'),\n",
       "  ('reduce', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('dust', 'NOUN'),\n",
       "  ('on', 'ADP'),\n",
       "  ('it', 'PRON'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('so', 'ADV'),\n",
       "  ('far', 'ADV'),\n",
       "  ('ok', 'ADJ'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('People', 'NOUN'),\n",
       "  ('can', 'AUX'),\n",
       "  ('choose', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('size', 'NOUN'),\n",
       "  ('that', 'ADP'),\n",
       "  ('you', 'PRON'),\n",
       "  ('want', 'VERB'),\n",
       "  ('as', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('choices', 'NOUN'),\n",
       "  ('are', 'VERB'),\n",
       "  ('various', 'ADJ'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Will', 'AUX'),\n",
       "  ('repeat', 'VERB'),\n",
       "  ('order', 'VERB'),\n",
       "  ('again', 'ADV'),\n",
       "  ('if', 'SCONJ'),\n",
       "  ('nice', 'ADJ')],\n",
       " [('So', 'ADV'), ('cute', 'ADJ')],\n",
       " [('Thank', 'VERB'),\n",
       "  ('you', 'PRON'),\n",
       "  ('seller', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('price', 'NOUN'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('good', 'ADJ'),\n",
       "  ('packaging', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('will', 'AUX'),\n",
       "  ('purchase', 'VERB'),\n",
       "  ('again', 'ADV'),\n",
       "  ('next', 'ADJ'),\n",
       "  ('time', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Item', 'NOUN'), ('arrived', 'VERB'), ('safe', 'ADJ'), ('.', 'PUNCT')],\n",
       " [('Functionality', 'NOUN'), (':', 'PUNCT'), ('Good', 'ADJ')],\n",
       " [('convenient', 'ADJ')],\n",
       " [('Thanks', 'VERB'), ('shop', 'NOUN'), ('so', 'ADV'), ('much', 'ADV')],\n",
       " [('Good', 'ADJ'),\n",
       "  ('condition', 'NOUN'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('recommended', 'VERB'),\n",
       "  ('Good', 'ADJ'),\n",
       "  ('condition', 'NOUN'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('recommended', 'VERB'),\n",
       "  ('Good', 'ADJ'),\n",
       "  ('condition', 'NOUN'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('recommended', 'VERB'),\n",
       "  ('Good', 'ADJ'),\n",
       "  ('condition', 'NOUN'),\n",
       "  ('and', 'CCONJ'),\n",
       "  ('recommended', 'VERB')],\n",
       " [('Quality', 'NOUN'), (':', 'PUNCT'), ('Good', 'ADJ')],\n",
       " [('Nice', 'ADJ'),\n",
       "  ('tq', 'X'),\n",
       "  ('Nice', 'ADJ'),\n",
       "  ('tq', 'X'),\n",
       "  ('Nice', 'ADJ'),\n",
       "  ('tq', 'X'),\n",
       "  ('Nice', 'ADJ'),\n",
       "  ('tq', 'X'),\n",
       "  ('Nice', 'ADJ'),\n",
       "  ('tq', 'X'),\n",
       "  ('Nice', 'ADJ'),\n",
       "  ('tq', 'X'),\n",
       "  ('Nice', 'ADJ'),\n",
       "  ('tq', 'X'),\n",
       "  ('Nice', 'ADJ'),\n",
       "  ('tq', 'X'),\n",
       "  ('Nice', 'ADJ'),\n",
       "  ('tq', 'X'),\n",
       "  ('Nice', 'ADJ'),\n",
       "  ('tq', 'X'),\n",
       "  ('Nice', 'ADJ'),\n",
       "  ('tq', 'X'),\n",
       "  ('Nice', 'ADJ'),\n",
       "  ('tq', 'X')],\n",
       " [('but', 'CCONJ'), ('dont', 'AUX'), ('worry', 'VERB'), ('.', 'PUNCT')],\n",
       " [('i', 'PRON'),\n",
       "  ('like', 'VERB'),\n",
       "  ('this', 'DET'),\n",
       "  ('satin', 'NOUN'),\n",
       "  ('pyjama', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('love', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('satin', 'NOUN'),\n",
       "  ('material', 'NOUN'),\n",
       "  ('soo', 'X'),\n",
       "  ('smooth', 'ADJ'),\n",
       "  ('to', 'PART'),\n",
       "  ('touch', 'VERB')],\n",
       " [('Tq', 'X'), ('seller', 'NOUN'), ('.', 'PUNCT')],\n",
       " [('Fast', 'ADJ'),\n",
       "  ('delivery', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('items', 'NOUN'),\n",
       "  ('received', 'VERB'),\n",
       "  ('as', 'ADP'),\n",
       "  ('per', 'ADP'),\n",
       "  ('my', 'PRON'),\n",
       "  ('order', 'NOUN'),\n",
       "  ('.', 'PUNCT')],\n",
       " [('Wow', 'INTJ'), ('!', 'PUNCT')],\n",
       " [(\"Can't\", 'AUX'),\n",
       "  ('wait', 'VERB'),\n",
       "  ('to', 'AUX'),\n",
       "  ('use', 'VERB'),\n",
       "  ('it', 'PRON'),\n",
       "  ('!', 'PUNCT'),\n",
       "  ('!', 'PUNCT'),\n",
       "  ('!', 'PUNCT')],\n",
       " [('I', 'PRON'),\n",
       "  ('have', 'AUX'),\n",
       "  ('been', 'AUX'),\n",
       "  ('waiting', 'VERB'),\n",
       "  ('for', 'ADP'),\n",
       "  ('this', 'DET'),\n",
       "  ('!', 'PUNCT'),\n",
       "  ('!', 'PUNCT'),\n",
       "  ('!', 'PUNCT')],\n",
       " [('Good', 'ADJ'), ('good', 'ADJ'), ('good', 'ADJ')],\n",
       " [('GOOD ', 'ADJ')],\n",
       " [('Recommend ', 'VERB')],\n",
       " [('Height ', 'NOUN'), (':', 'PUNCT'), ('159 ', 'NUM'), ('cm ', 'NOUN')],\n",
       " [('Mobile', 'PROPN'), ('legend', 'PROPN')],\n",
       " [('Kinda', 'INTJ'), ('love', 'VERB'), ('it', 'PRON')],\n",
       " [('$MENTION$', 'NOUN'),\n",
       "  ('Andersson', 'PROPN'),\n",
       "  ('the', 'DET'),\n",
       "  ('6p', 'NOUN'),\n",
       "  ('had', 'VERB'),\n",
       "  ('a', 'DET'),\n",
       "  ('1440p', 'NOUN'),\n",
       "  ('screen', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('type', 'NOUN'),\n",
       "  ('C', 'NOUN'),\n",
       "  ('usb', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('front', 'ADJ'),\n",
       "  ('stereo', 'NOUN'),\n",
       "  ('speakers', 'NOUN'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('all', 'DET'),\n",
       "  ('specs', 'NOUN'),\n",
       "  ('phones', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('time', 'NOUN'),\n",
       "  ('didnt', 'VERB'),\n",
       "  (',', 'PUNCT'),\n",
       "  ('loved', 'VERB'),\n",
       "  ('it', 'PRON')],\n",
       " [('Good', 'ADJ')],\n",
       " [('Goooooooooooooood', 'X')],\n",
       " [('W', 'X'), ('62', 'NUM'), ('kg', 'NOUN')],\n",
       " [('Thank', 'ADJ'), ('you', 'PRON'), ('seller', 'NOUN')],\n",
       " [('Fast', 'ADJ'), ('response', 'NOUN')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd65ba28-be86-413e-bba1-b852e3559a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 'DET')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f1af50c-998b-4ca2-b6bb-ee1c24ebc289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6318 entries, 0 to 6317\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tagged  6318 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 49.5+ KB\n"
     ]
    }
   ],
   "source": [
    "seniors_training_data = nltk_data\n",
    "std_df = pd.DataFrame({'tagged':seniors_training_data})\n",
    "std_df.info()\n",
    "# std_df.to_csv(\"seniors_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0f93a-995a-485a-a3e9-93e25b9dae74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "731ea9c5-0e80-4aaa-8b76-a4ce4d133dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_difference(hmm_result, crf_result):\n",
    "    if len(hmm_result) == len(crf_result):\n",
    "\n",
    "        arr_CRF = np.array(crf_result)\n",
    "        arr_HMM = np.array(hmm_result)\n",
    "\n",
    "        different_list = []\n",
    "        counter = 0 \n",
    "        different_tag = 0 \n",
    "        equal_tag = 0\n",
    "        arr_element = 0 \n",
    "\n",
    "        if (np.array_equal(arr_HMM, arr_CRF)) == False and (len(arr_CRF) == len(arr_HMM)):\n",
    "            arr_element = len(arr_CRF)\n",
    "\n",
    "            for j in range(arr_element):\n",
    "\n",
    "                if np.array_equal(arr_HMM[j], arr_CRF[j]) == False:\n",
    "                    # using * operator to concat\n",
    "                    temp =[*arr_HMM[j], *arr_CRF[j]]\n",
    "                    # del func duplicate comment in the list\n",
    "                    del temp[2]\n",
    "                    counter += 1 \n",
    "                    different_list.append(temp)\n",
    "            different_tag = counter\n",
    "            equal_tag = arr_element - different_tag\n",
    "    else:\n",
    "        different_list = []\n",
    "        different_tag = 0 \n",
    "        print(\"tag length is different\")\n",
    "            \n",
    "    return different_list, different_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "930ec3ce-a02b-4a77-a7a4-ff8a3f7e6257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5054\n",
      "1264\n"
     ]
    }
   ],
   "source": [
    "def features(sentence, index):\n",
    "    # \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    \n",
    "    feature_set =  {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'prefix-4': sentence[index][:4],\n",
    "        'prefix-5': sentence[index][:5],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'suffix-4': sentence[index][-4:],\n",
    "        'suffix-5': sentence[index][-5:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'prev2_word': '' if index == 0 else sentence[index - 2],\n",
    "        'next2_word': '' if index == len(sentence) - 2 or index == len(sentence) - 1  else sentence[index + 2],\n",
    "        'prev3_word': '' if index == 0 else sentence[index - 3],\n",
    "        'next3_word': '' if index == len(sentence) - 2 or index == len(sentence) - 1  or index == len(sentence) - 3  else sentence[index + 3],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "        'natural_number': (re.findall(r'^[0-9]+', sentence[index])),\n",
    "        'initcaps' : (re.findall(r'^[A-Z]\\w+', sentence[index])),\n",
    "        'initcapsalpha': (re.findall(r'^[A-Z][a-z]\\w+', sentence[index])),\n",
    "        'word.stemmed': re.sub(r'(.{2,}?)([aeiougyn]+$)',r'\\1', sentence[index].lower()),\n",
    "        'word.ispunctuation': (sentence[index] in string.punctuation)\n",
    "    }\n",
    "    \n",
    "    if index <= 0:\n",
    "        feature_set['BOS'] = True\n",
    "    \n",
    "    if index > len(sentence)-1:\n",
    "        feature_set['EOS'] = True\n",
    "        \n",
    "    return feature_set\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        X.append([features(untag(tagged), index) for index in range(len(tagged))])\n",
    "        y.append([tag for _, tag in tagged])\n",
    " \n",
    "    return X, y\n",
    "\n",
    "def pos_tag(sentence, model):\n",
    "    sentence = sentence_splitter(sentence)\n",
    "    sentence_features = [features(sentence, index) for index in range(len(sentence))]\n",
    "    return list(zip(sentence, model.predict([sentence_features])[0]))\n",
    "\n",
    "def sentence_splitter(sentence):\n",
    "    result = []\n",
    "    sents = word_tokenize(sentence)\n",
    "    for s in sents:\n",
    "        if re.findall(r'[\\u4e00-\\u9fff]+', s):\n",
    "            s = HanziConv.toSimplified(s)\n",
    "            result = result + list(jieba.cut(s, cut_all=False))\n",
    "        else:\n",
    "            result.append(s)\n",
    "    return result\n",
    "\n",
    "# Split the dataset for training and testing\n",
    "cutoff = int(.80 * len(nltk_data))\n",
    "training_sentences = nltk_data[:cutoff]\n",
    "test_sentences = nltk_data[cutoff:]\n",
    "\n",
    "X_train, y_train = transform_to_dataset(training_sentences)\n",
    "X_test, y_test = transform_to_dataset(test_sentences)\n",
    "\n",
    "# print(cutoff)\n",
    "print(len(X_train))     \n",
    "print(len(X_test))         \n",
    "# print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f95ce39a-df27-4b27-835f-d17e793dcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75bd3dab-fa2b-47f0-a332-57cb71b0c960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.25, c2=0.35,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CRF_model_pa = sklearn_crfsuite.CRF(\n",
    "#     algorithm = 'pa',\n",
    "#     max_iterations = 100,\n",
    "#     all_possible_transitions=True\n",
    "#     # c1 = 0.25,\n",
    "#     # c2 = 0.35\n",
    "# )\n",
    "\n",
    "# CRF_model_pa.fit(X_train, y_train)\n",
    "\n",
    "# CRF_model_ap = sklearn_crfsuite.CRF(\n",
    "#     algorithm = 'ap',\n",
    "#     max_iterations = 100,\n",
    "#     all_possible_transitions=True\n",
    "#     # c1 = 0.25,\n",
    "#     # c2 = 0.35\n",
    "# )\n",
    "# CRF_model_ap.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# CRF_model_arow = sklearn_crfsuite.CRF(\n",
    "#     algorithm = 'arow',\n",
    "#     max_iterations = 100,\n",
    "#     all_possible_transitions=True\n",
    "#     # c1 = 0.25,\n",
    "#     # c2 = 0.35\n",
    "# )\n",
    "# CRF_model_arow.fit(X_train, y_train)\n",
    "\n",
    "# CRF_model_l2sgd = sklearn_crfsuite.CRF(\n",
    "#     algorithm = 'l2sgd',\n",
    "#     max_iterations = 100,\n",
    "#     all_possible_transitions=True\n",
    "#     # c1 = 0.25,\n",
    "#     # c2 = 0.35\n",
    "# )\n",
    "# CRF_model_l2sgd.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "CRF_model_lbfgs = sklearn_crfsuite.CRF(\n",
    "    algorithm = 'lbfgs',\n",
    "    max_iterations = 100,\n",
    "    all_possible_transitions=True,\n",
    "    c1 = 0.25,\n",
    "    c2 = 0.35\n",
    ")\n",
    "CRF_model_lbfgs.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d25b6835-c469-4f3f-b302-2875cdc0a647",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT', 'PROPN', 'PART', 'VERB', 'ADV', 'CCONJ', 'AUX', 'PRON', 'X', 'INTJ', 'SCONJ', 'NUM', 'SYM']\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.5min\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1464: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1464: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=CRF(algorithm='lbfgs',\n",
       "                                 all_possible_transitions=True, c1=0.25,\n",
       "                                 c2=0.35, keep_tempfiles=None,\n",
       "                                 max_iterations=100),\n",
       "                   n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={'c1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f9ec66f7af0>,\n",
       "                                        'c2': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f9ec6543670>},\n",
       "                   scoring=make_scorer(flat_f1_score, average=weighted, labels=['ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT', 'PROPN', 'PART', 'VERB', 'ADV', 'CCONJ', 'AUX', 'PRON', 'X', 'INTJ', 'SCONJ', 'NUM', 'SYM']),\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(CRF_model_lbfgs.classes_)\n",
    "# labels.remove('O')\n",
    "print(labels)\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(CRF_model_lbfgs, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df06dcdb-643c-434e-aa18-605c7b3dde98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.6min\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1464: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:  7.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=CRF(algorithm='lbfgs', all_possible_transitions=True,\n",
       "                           c1=0.25, c2=0.35, keep_tempfiles=None,\n",
       "                           max_iterations=100),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'c1': [0, 0.05, 0.1, 0.25, 0.5, 1],\n",
       "                         'c2': [0, 0.05, 0.1, 0.25, 0.5, 1]},\n",
       "             scoring=make_scorer(flat_f1_score, average=weighted, labels=['ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT', 'PROPN', 'PART', 'VERB', 'ADV', 'CCONJ', 'AUX', 'PRON', 'X', 'INTJ', 'SCONJ', 'NUM', 'SYM']),\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params_space = {\n",
    "    \"c1\": [0,0.05,0.1, 0.25,0.5,1],\n",
    "    \"c2\": [0,0.05,0.1, 0.25,0.5,1]\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "grid_search = GridSearchCV(estimator=CRF_model_lbfgs,\n",
    "                           param_grid=params_space,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1, verbose=1,scoring=f1_scorer)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e757b2ab-fec8-4026-9c47-2c54481389f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3298b910-58df-4b45-a813-ade2c6438dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF_model_lbfgs :  96.9008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass labels=['X', 'PART', 'CCONJ', 'SCONJ', 'ADJ', 'ADP', 'ADV', 'VERB', 'DET', 'INTJ', 'NOUN', 'PRON', 'PROPN', 'NUM', 'PUNCT', 'AUX', 'SYM'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           X      0.997     1.000     0.998      2127\n",
      "        PART      0.996     1.000     0.998       991\n",
      "       CCONJ      0.999     0.996     0.997       683\n",
      "       SCONJ      0.000     0.000     0.000         0\n",
      "         ADJ      0.868     0.897     0.883      1988\n",
      "         ADP      0.982     0.982     0.982      3153\n",
      "         ADV      0.866     0.878     0.872       793\n",
      "        VERB      0.967     0.962     0.965      4206\n",
      "         DET      0.991     0.994     0.992      2837\n",
      "        INTJ      0.000     0.000     0.000         0\n",
      "        NOUN      0.955     0.949     0.952      6313\n",
      "        PRON      0.994     1.000     0.997       479\n",
      "       PROPN      0.972     0.961     0.966      2918\n",
      "         NUM      0.999     0.994     0.997      1599\n",
      "       PUNCT      1.000     1.000     1.000      3273\n",
      "         AUX      0.000     0.000     0.000         0\n",
      "         SYM      1.000     1.000     1.000       326\n",
      "\n",
      "   micro avg      0.969     0.969     0.969     31686\n",
      "   macro avg      0.799     0.801     0.800     31686\n",
      "weighted avg      0.969     0.969     0.969     31686\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_search.predict(X_test)\n",
    "print(\"CRF_model_lbfgs : \", round(metrics.flat_accuracy_score(y_test, y_pred)*100, 4))\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39aa7b6-18fe-410b-a66d-eadebeec6807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "y_pred_lbfgs = CRF_model_lbfgs.predict(X_test)\n",
    "print(\"CRF_model_lbfgs : \", round(metrics.flat_accuracy_score(y_test, y_pred_lbfgs)*100, 4))\n",
    "print(\"CRF_model_lbfgs    : \\n\", metrics.flat_classification_report(y_test, y_pred_lbfgs), \"\\n\")\n",
    "\n",
    "# y_pred_l2sgd = CRF_model_l2sgd.predict(X_test)\n",
    "# print(\"CRF_model_l2sgd : \", round(metrics.flat_accuracy_score(y_test, y_pred_l2sgd)*100, 4))\n",
    "# print(\"CRF_model_l2sgd    : \\n\", metrics.flat_classification_report(y_test, y_pred_l2sgd), \"\\n\")\n",
    "\n",
    "# y_pred_arow = CRF_model_arow.predict(X_test)\n",
    "# print(\"CRF_model_arow  : \", round(metrics.flat_accuracy_score(y_test, y_pred_arow)*100, 4))\n",
    "# print(\"CRF_model_arow    : \\n\", metrics.flat_classification_report(y_test, y_pred_arow), \"\\n\")\n",
    "\n",
    "# y_pred_pa = CRF_model_pa.predict(X_test)\n",
    "# print(\"CRF_model_pa    : \", round(metrics.flat_accuracy_score(y_test, y_pred_pa)*100, 4))\n",
    "# print(\"CRF_model_pa    : \\n\", metrics.flat_classification_report(y_test, y_pred_pa), \"\\n\")\n",
    "\n",
    "# y_pred_ap = CRF_model_ap.predict(X_test)\n",
    "# print(\"CRF_model_ap    : \", round(metrics.flat_accuracy_score(y_test, y_pred_ap)*100, 4))\n",
    "# print(\"CRF_model_ap    : \\n\", metrics.flat_classification_report(y_test, y_pred_ap), \"\\n\")\n",
    "\n",
    "# print(metrics.sequence_accuracy_score(y_test, y_pred_lbfgs)*100)\n",
    "# https://sklearn-crfsuite.readthedocs.io/en/latest/_modules/sklearn_crfsuite/metrics.html\n",
    "# CRF_model_lbfgs :  97.664\n",
    "# CRF_model_l2sgd :  97.3046\n",
    "# CRF_model_arow  :  99.7604\n",
    "# CRF_model_pa    :  99.7904\n",
    "# CRF_model_ap    :  99.8203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b94d8-c87b-4e1b-a8d0-dae6de2c9bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CRF_model_lbfgs : \", round(metrics.flat_accuracy_score(y_test, y_pred_lbfgs)*100, 4))\n",
    "# print(\"CRF_model_l2sgd : \", round(metrics.flat_accuracy_score(y_test, y_pred_l2sgd)*100, 4))\n",
    "# print(\"CRF_model_arow  : \", round(metrics.flat_accuracy_score(y_test, y_pred_arow)*100, 4))\n",
    "# print(\"CRF_model_pa    : \", round(metrics.flat_accuracy_score(y_test, y_pred_pa)*100, 4))\n",
    "# print(\"CRF_model_ap    : \", round(metrics.flat_accuracy_score(y_test, y_pred_ap)*100, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4dbe80-49e0-48fd-ad2c-a5de023e134b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence = \"semalan im i makan jadi so happy dengan holiday\"\n",
    "# Arang sampai dgn keadaan cantikkk\n",
    "# Malas nak amik gambar hehe\n",
    "\n",
    "crf_result_lbfgs = pos_tag(sentence, CRF_model_lbfgs)\n",
    "\n",
    "print(\"CRF lbfgs :\", crf_result_lbfgs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55aa0c-3bb6-4d47-b218-30c32a677f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re \n",
    "import preprocessor as p\n",
    "\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    \n",
    "    combine = [] \n",
    "    token_list = []\n",
    "    full_sent = \"\"\n",
    "    sent = \"\"\n",
    "    sent_token = sent_tokenize(text)\n",
    "    \n",
    "    for sentence in sent_token: \n",
    "        reg_sent_token = re.split('\\.|\\;{2,}', sentence)\n",
    "        # print(reg_sent_token)\n",
    "        \n",
    "        for reg_sentence in reg_sent_token:\n",
    "            if reg_sentence != '':\n",
    "                reg_sentence = reg_sentence.strip()\n",
    "                token_list.append(reg_sentence) \n",
    "\n",
    "#     sentence token \n",
    "    return token_list\n",
    "\n",
    "def unicode_problem(text):\n",
    "    return re.sub(r'[\\u0080]','',text).strip()\n",
    "\n",
    "## Remove special character (\\r)\n",
    "def remove_slashR(text):\n",
    "    return re.sub('\\r', ' ', text)\n",
    "\n",
    "def remove_special_char(text):\n",
    "    return re.sub('&quot;|\"|“|”', '', text)\n",
    "\n",
    "def remove_multiple_space(text):\n",
    "    return re.sub('\\s+', ' ', text).strip()\n",
    "\n",
    "# Replace newline\n",
    "def remove_newline(text):\n",
    "    return re.sub(r'\\n', ' ', text)\n",
    "\n",
    "# Replace apostrophe's special characters to original apostrophe\n",
    "def replace_apostrophes(text):\n",
    "    return re.sub('&#39;|’|´|‘', \"'\", text)\n",
    "\n",
    "def remove_multiple_comma(text):\n",
    "    return re.sub(r'[,]{2,}',',', text)\n",
    "\n",
    "def remove_multiple_dot(text):\n",
    "    return re.sub(r'[.]{3,}','', text)\n",
    "\n",
    "def preprocess_tweet_fn(text):\n",
    "    p.set_options(p.OPT.URL, p.OPT.HASHTAG, p.OPT.MENTION)\n",
    "    return p.tokenize(text) \n",
    "\n",
    "## Convert traditional Chinese characters to Simplified Chinese characters\n",
    "def convert_Tra_Simp_Chi(text):\n",
    "    return HanziConv.toSimplified(text)\n",
    "\n",
    "def text_precessing(text):\n",
    "    textlist = []\n",
    "    \n",
    "    text = str(text)\n",
    "    text = sentence_tokenize(text)\n",
    "    for i in text:\n",
    "        i = unicode_problem(i)\n",
    "        i = remove_slashR(i)\n",
    "        i = remove_special_char(i)\n",
    "        i = remove_multiple_space(i)\n",
    "        i = remove_newline(i)\n",
    "        i = replace_apostrophes(i)\n",
    "        i = remove_multiple_comma(i)\n",
    "        i = remove_multiple_dot(i)\n",
    "        i = preprocess_tweet_fn(i)\n",
    "        i = convert_Tra_Simp_Chi(i)\n",
    "        \n",
    "        textlist.append(i)\n",
    "    return textlist \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584f2b1-0654-4182-91ff-1e25145ea7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"tems received with good condition. Thanks.       \"\n",
    "text_precessing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a894851-c1df-445c-bcb6-8854a3451617",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df_Alldata['Sentence'].sample(n=5000, random_state=40)\n",
    "\n",
    "sentence_list = []\n",
    "\n",
    "for i in test_df:\n",
    "    sentence_token = text_precessing(i)\n",
    "    for s in sentence_token:\n",
    "        if s != '':\n",
    "            sentence_list.append(s)\n",
    "\n",
    "            \n",
    "data = {'sentence':sentence_list\n",
    "       }   \n",
    "new_df = pd.DataFrame(data)\n",
    "sentence_df = new_df['sentence']\n",
    "sentence_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dcd812-f79d-4a05-be7c-7f78c36abfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRF_model_pa\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# test_df = df_Alldata['Sentence'].sample(n=5000, random_state=42)\n",
    "\n",
    "\n",
    "# pred_tags_HMM_List = []\n",
    "pred_tags_CRF_List_lbfgs = []\n",
    "pred_tags_CRF_List_l2sgd = []\n",
    "pred_tags_CRF_List_arow = []\n",
    "pred_tags_CRF_List_pa = []\n",
    "pred_tags_CRF_List_ap = []\n",
    "\n",
    "different_tag_List = []\n",
    "different_num_List = []\n",
    "\n",
    "\n",
    "for i in sentence_df: \n",
    "    test_sent = i.lower()\n",
    "    \n",
    "    crf_result_pa = pos_tag(test_sent, CRF_model_lbfgs)\n",
    "\n",
    "    pred_tags_CRF_List_lbfgs.append(crf_result_pa)\n",
    "\n",
    "\n",
    "data = {'sentence':sentence_df,\n",
    "        'CRF plbfgs':pred_tags_CRF_List_lbfgs\n",
    "       }   \n",
    "    \n",
    "\n",
    "postag_df = pd.DataFrame(data)\n",
    "\n",
    "postag_df = postag_df.drop_duplicates(subset=['sentence'])\n",
    "postag_df.to_csv(\"CRF pos tag (lbfgs).csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2428f937-fd46-4bd1-bddf-84782db76373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"CRF pos tag (lbfgs).csv\")\n",
    "total = len(df)\n",
    "divide = int(total/2)\n",
    "print(total, divide)\n",
    "df1 = df.iloc[:divide]\n",
    "df2 = df.iloc[divide:]\n",
    "df1.to_csv(\"CRF pos tag 1.csv\", index = False)\n",
    "# df2.to_csv(\"CRF pos tag 2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cff128-7259-4a21-b947-936a160be319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# test_df = df_Alldata['Sentence'].sample(n=5000, random_state=42)\n",
    "\n",
    "# # pred_tags_HMM_List = []\n",
    "# pred_tags_CRF_List_lbfgs = []\n",
    "# pred_tags_CRF_List_l2sgd = []\n",
    "# pred_tags_CRF_List_arow = []\n",
    "# pred_tags_CRF_List_pa = []\n",
    "# pred_tags_CRF_List_ap = []\n",
    "\n",
    "# different_tag_List = []\n",
    "# different_num_List = []\n",
    "\n",
    "# for i in test_df: \n",
    "#     test_sent = i\n",
    "    \n",
    "#     # hmm_result = Viterbi(sentence_splitter(test_sent))\n",
    "#     crf_result_lbfgs = pos_tag(test_sent, CRF_model_lbfgs)\n",
    "#     crf_result_l2sgd = pos_tag(test_sent, CRF_model_l2sgd)\n",
    "#     crf_result_arow = pos_tag(test_sent, CRF_model_arrow)\n",
    "#     crf_result_pa = pos_tag(test_sent, CRF_model_pa)\n",
    "#     crf_result_ap = pos_tag(test_sent, CRF_model_ap)\n",
    "    \n",
    "#     different_list, different_num = check_difference(crf_result_lbfgs, crf_result_l2sgd)\n",
    "    \n",
    "#     # pred_tags_HMM_List.append(hmm_result)\n",
    "#     pred_tags_CRF_List_lbfgs.append(crf_result_lbfgs)\n",
    "#     pred_tags_CRF_List_l2sgd.append(crf_result_l2sgd)\n",
    "#     pred_tags_CRF_List_arow.append(crf_result_arow)\n",
    "#     pred_tags_CRF_List_pa.append(crf_result_pa)\n",
    "#     pred_tags_CRF_List_ap.append(crf_result_ap)\n",
    "#     different_num_List.append(different_num)\n",
    "#     different_tag_List.append(different_list)\n",
    "\n",
    "    \n",
    "    \n",
    "# data = {'sentence':test_df,\n",
    "#         'CRF lbfgs':pred_tags_CRF_List_lbfgs,\n",
    "#         'CRF l2sgd':pred_tags_CRF_List_l2sgd,\n",
    "#         'No. of difference lbfgs vs l2sgd' : different_num_List,\n",
    "#         'Different Tags of lbfgs vs l2sgd': different_tag_List,\n",
    "#         'CRF arow':pred_tags_CRF_List_arow,\n",
    "#         'CRF pa':pred_tags_CRF_List_pa,\n",
    "#         'CRF ap':pred_tags_CRF_List_ap}\n",
    "#         # 'HMM':pred_tags_HMM_List,\n",
    "#         # 'No. of difference':different_num_List,\n",
    "#         # 'Different Tags': different_tag_List}\n",
    "\n",
    "# new_df = pd.DataFrame(data)\n",
    "# new_df\n",
    "\n",
    "# new_df.to_csv(\"CRF parameters.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a666eb7e-f541-4a44-9cc4-1753f0c84a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(CRF_model_lbfgs.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(CRF_model_lbfgs.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13dc09-78d4-4f2b-9594-77152496328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(CRF_model_lbfgs.state_features_).most_common(30))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(CRF_model_lbfgs.state_features_).most_common()[-30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa23d4-95a7-41ff-945e-1438bbb6da40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8e7e1a-a01c-44fd-93e3-3259a576b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(CRF_model_lbfgs, top=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1e7f99-42f7-4ed3-98ce-76f54a9dada3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melex-kernel",
   "language": "python",
   "name": "melex-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
