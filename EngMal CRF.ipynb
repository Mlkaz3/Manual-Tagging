{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52fe6351-b37a-4485-8773-dda013659f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time\n",
    "\n",
    "import sklearn\n",
    "import sklearn_crfsuite\n",
    "import scipy.stats\n",
    "import math, string, re\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn_crfsuite import CRF\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag.util import untag\n",
    "\n",
    "import ast\n",
    "from ast import literal_eval\n",
    "\n",
    "import jieba \n",
    "from hanziconv import HanziConv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from operator import add\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import preprocessor as p\n",
    "import pkuseg\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10118c5c-378a-45f0-8ff4-353c819e1ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../../Corpus/English/treebank_Mapped.txt\", \"r\")\n",
    "\n",
    "treebank_Mapped_list = []\n",
    "for l in f:\n",
    "    l = literal_eval(l)\n",
    "    treebank_Mapped_list.append(l)\n",
    "f.close()\n",
    "\n",
    "f = open(\"../../Corpus/English/Brown_Mapped/part-00000\", \"r\")\n",
    "\n",
    "Brown_Mapped_list = []\n",
    "for l in f:\n",
    "    l = literal_eval(l)\n",
    "    Brown_Mapped_list.append(l)\n",
    "f.close()\n",
    "\n",
    "f = open(\"../../Data/Sentence_Chinese/chinese_postag_sentence.txt\", \"r\")\n",
    "\n",
    "chinese_postag_sentence = []\n",
    "for l in f:\n",
    "    l = literal_eval(l)\n",
    "    chinese_postag_sentence.append(l)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da091686-89e9-457e-a28d-377be6fd6f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61254\n"
     ]
    }
   ],
   "source": [
    "engmal_nltk_list = treebank_Mapped_list + Brown_Mapped_list \n",
    "print(len(engmal_nltk_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98e63a9a-0e42-4873-95c3-29d2aed5bc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178300\n"
     ]
    }
   ],
   "source": [
    "df_Alldata = pd.read_csv('../../Data/Sample_Data/Sample_All.csv')\n",
    "df_training = pd.read_csv(r'../manuallyTagging.csv')\n",
    "# df_training = pd.read_csv(r'../testing posTAG.csv')\n",
    "\n",
    "duplicates = pd.merge(df_Alldata, df_training, how='inner',\n",
    "                  left_on=['Sentence'], right_on=['f'])\n",
    "\n",
    "# drop the indices from USERS\n",
    "df_Alldata = df_Alldata.drop(duplicates.index)\n",
    "\n",
    "df_Alldata = df_Alldata.drop(['Unnamed: 0', 'Key', 'token_words'], axis = 1)\n",
    "df_Alldata = df_Alldata.dropna()\n",
    "print(len(df_Alldata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7724c5-e508-42c8-b1bd-cc90579a35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'../testing posTAG.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59235380-9574-411d-bb65-b752e94ccf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['f', 'Pos Tag'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa9eb3f-cfdd-4727-9105-0684e62634cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2404\n",
      "0\n",
      "1765\n"
     ]
    }
   ],
   "source": [
    "df_withoutchinese = pd.read_csv(r'../testing posTAG.csv')\n",
    "df_withoutchinese = df_withoutchinese.dropna()\n",
    "\n",
    "print(len(df_withoutchinese))\n",
    "counter = 0 \n",
    "for i in df_withoutchinese['f']:\n",
    "    if re.findall(r'[\\u4e00-\\u9fff]', i):\n",
    "        df_withoutchinese = df_withoutchinese.drop(df_withoutchinese.index[df_withoutchinese['f'] == i])\n",
    "        # print(i)\n",
    "\n",
    "print(counter)\n",
    "print(len(df_withoutchinese))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d386e585-d4df-4f74-8442-2804b32472a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2404\n",
      "1765\n"
     ]
    }
   ],
   "source": [
    "def remove_newline(text):\n",
    "    return re.sub(r'\\n', ' ', str(text))\n",
    "\n",
    "def find_astrophe(text):\n",
    "    return bool(re.findall(r'\\'', text))\n",
    "\n",
    "def convert_string2_list(text):\n",
    "\n",
    "    text = ast.literal_eval(str(text))\n",
    "    return text\n",
    "\n",
    "def unicode_problem(text):\n",
    "    return re.sub(r'[\\u0080]','',text).strip()\n",
    "\n",
    "def unprinable(text):\n",
    "     return re.sub(r'[\\u200B]','',text).strip()\n",
    "    \n",
    "def replace_apostrophes(text):\n",
    "    return re.sub('&#39;|’|´|‘', \"'\", (str(text)))\n",
    "\n",
    "\n",
    "# df = pd.read_csv(r'../testing posTAG.csv')\n",
    "# df = df.dropna()\n",
    "\n",
    "\n",
    "templist = [] \n",
    "templist_withoutchinse = []\n",
    "\n",
    "for i in df[\"Pos Tag\"]:\n",
    "    i = remove_newline(i)\n",
    "    i = unicode_problem(i)\n",
    "    i = unprinable(i)\n",
    "    i = replace_apostrophes(i)\n",
    "    i = i.strip()\n",
    "    i = convert_string2_list(i)\n",
    "    \n",
    "    templist.append(i)\n",
    "\n",
    "print(len(templist))\n",
    "\n",
    "\n",
    "for i in df_withoutchinese[\"Pos Tag\"]:\n",
    "    i = remove_newline(i)\n",
    "    i = unicode_problem(i)\n",
    "    i = unprinable(i)\n",
    "    i = replace_apostrophes(i)\n",
    "    i = i.strip()\n",
    "    i = convert_string2_list(i)\n",
    "    \n",
    "    templist_withoutchinse.append(i)\n",
    "\n",
    "print(len(templist_withoutchinse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57be2fa6-fe71-4504-826d-5be9c3962ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2404\n",
      "1765\n"
     ]
    }
   ],
   "source": [
    "# process_data = templist + nltk_list\n",
    "process_data = templist\n",
    "\n",
    "process_data_withoutchinese = templist_withoutchinse\n",
    "print(len(process_data))\n",
    "print(len(process_data_withoutchinese))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8258e58-61d9-42af-b827-a42fbc175cad",
   "metadata": {},
   "source": [
    "## CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02cb93f4-7530-43e3-943a-87c5340d43bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "    # \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    isChinese = False\n",
    "    \n",
    "  \n",
    "    feature_set = {\n",
    "        'word': sentence[index],\n",
    "        'isChinese' : isChinese, \n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'prefix-4': sentence[index][:4],\n",
    "        'prefix-5': sentence[index][:5],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'suffix-4': sentence[index][-4:],\n",
    "        'suffix-5': sentence[index][-5:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'prev2_word': '' if index == 0 else sentence[index - 2],\n",
    "        'next2_word': '' if index == len(sentence) - 2 or index == len(sentence) - 1  else sentence[index + 2],\n",
    "        'prev3_word': '' if index == 0 else sentence[index - 3],\n",
    "        'next3_word': '' if index == len(sentence) - 2 or index == len(sentence) - 1  or index == len(sentence) - 3  else sentence[index + 3],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "        'natural_number': bool(re.findall(r'^[0-9]+', sentence[index])),\n",
    "        'initcaps' : bool(re.findall(r'^[A-Z]\\w+', sentence[index])),\n",
    "        'initcapsalpha': bool(re.findall(r'^[A-Z][a-z]\\w+', sentence[index])),\n",
    "        'word.stemmed': re.sub(r'(.{2,}?)([aeiougyn]+$)',r'\\1', sentence[index].lower()),\n",
    "        'word.ispunctuation': (sentence[index] in string.punctuation),\n",
    "        'contain_digit': any(map(str.isdigit, sentence[index])),\n",
    "        'prev_word_prefix_1': '' if index == 0 else sentence[index - 1][0],\n",
    "        'prev_word_prefix_2': '' if index == 0 else sentence[index - 1][:2],\n",
    "        'prev_word_prefix_3': '' if index == 0 else sentence[index - 1][:3],\n",
    "        'prev_word_prefix_4': '' if index == 0 else sentence[index - 1][:4],\n",
    "        'next_word_prefix_1': '' if index == len(sentence) - 1 else sentence[index + 1][0] ,\n",
    "        'next_word_prefix_2': '' if index == len(sentence) - 1 else sentence[index + 1][:2],\n",
    "        'next_word_prefix_3': '' if index == len(sentence) - 1 else sentence[index + 1][:3],\n",
    "        'next_word_prefix_4': '' if index == len(sentence) - 1 else sentence[index + 1][:4],\n",
    "        \n",
    "         # 'prev_word_suffix_1': '' if index == 0 else sentence[index - 1][-1],\n",
    "        # 'prev_word_suffix_2': '' if index == 0 else sentence[index - 1][-2],\n",
    "        # 'prev_word_suffix_3': '' if index == 0 else sentence[index - 1][-3],\n",
    "        # 'prev_word_suffix_4': '' if index == 0 else sentence[index - 1][-4],\n",
    "        # 'next_word_suffix_1': '' if index == len(sentence) - 1 else sentence[index + 1][-1],\n",
    "        # 'next_word_suffix_2': '' if index == len(sentence) - 1 else sentence[index + 1][-2],\n",
    "        # 'next_word_suffix_3': '' if index == len(sentence) - 1 else sentence[index + 1][-3],\n",
    "        # 'next_word_suffix_4': '' if index == len(sentence) - 1 else sentence[index + 1][-4]\n",
    "        \n",
    "    }               \n",
    "        # 'formal_word' : 'True' if sentence[index].lower() in lexicon_lower_set else 'False',\n",
    "           # 'is_shortfrom' : check_shortform_namedentity(sentence[index], 1),\n",
    "        # 'is_namedentity' : check_shortform_namedentity(sentence[index], 2)\n",
    "\n",
    "\n",
    "    \n",
    "    if index <= 0:\n",
    "        feature_set['BOS'] = True\n",
    "    \n",
    "    if index > len(sentence)-1:\n",
    "        feature_set['EOS'] = True\n",
    "        \n",
    "    return feature_set\n",
    "                      \n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        X.append([features(untag(tagged), index) for index in range(len(tagged))])\n",
    "        y.append([tag for _, tag in tagged])\n",
    " \n",
    "    return X, y\n",
    "\n",
    "def pos_tag(sentence, model, seg):\n",
    "    sentence = sentence_splitter(sentence, seg)\n",
    "    sentence_features = [features(sentence, index) for index in range(len(sentence))]\n",
    "    return list(zip(sentence, model.predict([sentence_features])[0]))\n",
    "\n",
    "def wordTokenize(sentence):\n",
    "    \n",
    "    result = [] \n",
    "    nextValue = -1\n",
    "    punct_start = -1\n",
    "    doubleTokenize = True\n",
    "    counter = 0 \n",
    "    temp_string = \"\"\n",
    "    \n",
    "    combine_punct = bool(re.findall(r'《.*》| “.*”', sentence))\n",
    "    # print(combine_punct)\n",
    "    if combine_punct == True: \n",
    "        sentence = re.sub(r'(《)','  《' ,sentence)\n",
    "        sentence = re.sub(r'(》)','》 ' ,sentence)\n",
    "        sentence = re.sub(r'(“)','  “' ,sentence)\n",
    "        sentence = re.sub(r'(”)','” ' ,sentence)\n",
    "    \n",
    "    text = sentence.split()\n",
    "    # print(text)\n",
    "    for index, t in enumerate(text):\n",
    "        # print(t)\n",
    "        string = t\n",
    "        t = t.lower()\n",
    " \n",
    "        if nextValue != -1:\n",
    "            if nextValue == index:\n",
    "                continue\n",
    "            \n",
    "        if index != 0 and index != (len(text)-1): \n",
    "            \n",
    "            if t == '/':\n",
    "                \n",
    "                prevWord = text[index - 1]\n",
    "                nextWord = text[index + 1]\n",
    "                \n",
    "                if prevWord.isdigit() and nextWord.isdigit():\n",
    "                    string = text[index - 1] + t + text[index + 1]\n",
    "                    resultlen = len(result)\n",
    "                    if resultlen != 0:\n",
    "                        result.pop(resultlen - 1)\n",
    "            \n",
    "                    nextValue = index + 1 \n",
    "                    doubleTokenize = False\n",
    "                    \n",
    "        if re.findall(r'^rm\\d{1,}', t):\n",
    "    \n",
    "            rm = t[:2]\n",
    "            content = t[2:]\n",
    "            if re.findall(r'[a-zA-Z]+', content):\n",
    "                number = re.findall(r'[0-9]+', content)[0]\n",
    "                temp = re.split(r'[0-9]+', content)\n",
    "              \n",
    "                for t in temp:\n",
    "                    temp_string += t \n",
    "                    temp_string += \" \"\n",
    "                content = temp_string\n",
    "                \n",
    "                string = rm + ' ' + number + ' ' + content\n",
    "            else: \n",
    "                string = rm + ' ' + content\n",
    "            doubleTokenize = False\n",
    "        \n",
    "        if re.findall(r'^[~]+', t):\n",
    "            if re.findall(r'[a-zA-Z]+', t):\n",
    "                punctuation = re.findall(r'^[~]+', t)[0]\n",
    "                word = re.findall(r'[a-zA-Z]+', t)[0]\n",
    "                \n",
    "                string = punctuation + ' ' + word\n",
    "                # print(string)\n",
    "            doubleTokenize = False\n",
    "            \n",
    "    \n",
    "        if doubleTokenize == True:\n",
    "        \n",
    "            if (string.startswith(\"$\") and string.endswith(\"$\")):\n",
    "                result.append(string)\n",
    "                \n",
    "            else:\n",
    "                nltk_tokenize = word_tokenize(string)\n",
    "                if len(nltk_tokenize) > 1:\n",
    "                    for i in nltk_tokenize:\n",
    "                        result.append(i)\n",
    "                else:\n",
    "                    result.append(string)\n",
    "        else:\n",
    "            string = string.split()\n",
    "            for s in string:\n",
    "                result.append(s)\n",
    "    \n",
    "            \n",
    "    return result\n",
    "\n",
    "def sentence_splitter(sentence, seg):\n",
    "    result = []\n",
    "    sents = wordTokenize(sentence)\n",
    "    # print(sents)\n",
    "    for s in sents:\n",
    "        if re.findall(r'[\\u4e00-\\u9fff]+', s):\n",
    "            s = HanziConv.toSimplified(s)\n",
    "            if (s.startswith((\"《\")) and s.endswith((\"》\"))):\n",
    "                result.append(s)\n",
    "            elif (s.startswith((\"“\")) and s.endswith((\"”\"))):\n",
    "                result.append(s)\n",
    "            else:\n",
    "                result = result + list(seg.cut(s))\n",
    "        else:\n",
    "            result.append(s)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a567ce8-a39a-4e74-8f57-133dd7f24421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1923\n",
      "1847\n",
      "451\n"
     ]
    }
   ],
   "source": [
    "process_data_set = {tuple(sent_list) for sent_list in process_data}\n",
    "\n",
    "nltk_list_set = {tuple(sent_list) for sent_list in engmal_nltk_list}\n",
    "\n",
    "cutoff = int(.80 * len(process_data))\n",
    "training_sentences = random.sample(process_data, k = cutoff)\n",
    "training_sentence_set = {tuple(sent_list) for sent_list in training_sentences}\n",
    "\n",
    "test_sentences_set = {}\n",
    "test_sentences_set = set()\n",
    "\n",
    "for x in process_data_set:\n",
    "    if x not in training_sentence_set:\n",
    "        test_sentences_set.add(x)\n",
    "\n",
    "# combine_training_set = training_sentence_set.union(nltk_list_set)\n",
    "\n",
    "X_train, y_train = transform_to_dataset(training_sentence_set)\n",
    "X_test, y_test = transform_to_dataset(test_sentences_set)\n",
    "\n",
    "print(cutoff)\n",
    "print(len(X_train))     \n",
    "print(len(X_test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cda0a658-037b-4106-9d60-f4b5d140ca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.25, c2=0.35,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRF_model_lbfgs = sklearn_crfsuite.CRF(\n",
    "    algorithm = 'lbfgs',\n",
    "    max_iterations = 100,\n",
    "    all_possible_transitions=True,\n",
    "    c1 = 0.25,\n",
    "    c2 = 0.35\n",
    ")\n",
    "CRF_model_lbfgs.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "094a7421-a7eb-4457-bfc5-ea284feb9ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.25, c2=0.35,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRF_model_lbfgs.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc47c11d-fb0a-4d97-9c4a-57b4d742457b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF_model_lbfgs (word level) :  81.1594\n",
      "CRF_model_lbfgs (sentence level) :  37.0288\n",
      "CRF_model_lbfgs    : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.87      0.82      0.84       679\n",
      "         ADP       0.91      0.87      0.89       181\n",
      "         ADV       0.82      0.75      0.79       353\n",
      "         AUX       0.45      0.28      0.35        46\n",
      "       CCONJ       0.95      0.85      0.89        84\n",
      "         DET       0.87      0.93      0.90       119\n",
      "        INTJ       0.70      0.41      0.52        39\n",
      "        NOUN       0.76      0.84      0.80       885\n",
      "         NUM       0.76      0.82      0.79        67\n",
      "        PART       0.77      0.80      0.78       111\n",
      "        PRON       0.98      0.87      0.92       183\n",
      "       PROPN       0.36      0.55      0.43        82\n",
      "       PUNCT       0.96      0.98      0.97       456\n",
      "       SCONJ       0.67      0.62      0.64        39\n",
      "         SYM       0.20      0.03      0.05        37\n",
      "        VERB       0.79      0.83      0.81       658\n",
      "           X       0.71      0.59      0.65       190\n",
      "\n",
      "    accuracy                           0.81      4209\n",
      "   macro avg       0.74      0.70      0.71      4209\n",
      "weighted avg       0.81      0.81      0.81      4209\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## with nltk data \n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "y_pred_lbfgs = CRF_model_lbfgs.predict(X_test)\n",
    "print(\"CRF_model_lbfgs (word level) : \", round(metrics.flat_accuracy_score(y_test, y_pred_lbfgs)*100, 4))\n",
    "print(\"CRF_model_lbfgs (sentence level) : \", round(metrics.sequence_accuracy_score(y_test, y_pred_lbfgs)*100, 4))\n",
    "print(\"CRF_model_lbfgs    : \\n\", metrics.flat_classification_report(y_test, y_pred_lbfgs), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff010b-ac62-4a9e-8c5c-92fabccd7cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbe96f7f-e69d-4bdf-95b8-92c6f387b8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF_model_lbfgs (word level) :  84.7656\n",
      "CRF_model_lbfgs (sentence level) :  49.1071\n",
      "CRF_model_lbfgs    : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.85      0.84      0.84       339\n",
      "         ADP       0.93      0.90      0.91       163\n",
      "         ADV       0.86      0.80      0.83       163\n",
      "         AUX       0.60      0.48      0.54        31\n",
      "       CCONJ       0.98      0.97      0.98        65\n",
      "         DET       0.94      0.92      0.93       129\n",
      "        INTJ       0.50      0.30      0.38        23\n",
      "        NOUN       0.81      0.88      0.85       536\n",
      "         NUM       0.83      0.81      0.82        37\n",
      "        PART       0.39      0.50      0.44        14\n",
      "        PRON       0.99      0.93      0.96       147\n",
      "       PROPN       0.44      0.58      0.50        48\n",
      "       PUNCT       0.98      1.00      0.99       255\n",
      "       SCONJ       0.00      0.00      0.00         9\n",
      "         SYM       0.00      0.00      0.00        12\n",
      "        VERB       0.84      0.89      0.86       383\n",
      "           X       0.75      0.65      0.70       206\n",
      "\n",
      "    accuracy                           0.85      2560\n",
      "   macro avg       0.69      0.67      0.68      2560\n",
      "weighted avg       0.84      0.85      0.84      2560\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## without  nltk data \n",
    "\n",
    "process_data_set = {tuple(sent_list) for sent_list in process_data_withoutchinese}\n",
    "\n",
    "cutoff = int(.80 * len(process_data))\n",
    "\n",
    "# training_sentences = process_data[:cutoff]\n",
    "# test_sentences = process_data[cutoff:]\n",
    "\n",
    "training_sentences = random.sample(process_data, k = cutoff)\n",
    "training_sentence_set = {tuple(sent_list) for sent_list in training_sentences}\n",
    "\n",
    "test_sentences_set = {}\n",
    "test_sentences_set = set()\n",
    "\n",
    "for x in process_data_set:\n",
    "    if x not in training_sentence_set:\n",
    "        test_sentences_set.add(x)\n",
    "\n",
    "X_train, y_train = transform_to_dataset(training_sentence_set)\n",
    "X_test, y_test = transform_to_dataset(test_sentences_set)\n",
    "\n",
    "\n",
    "CRF_model_lbfgs.fit(X_train, y_train)  \n",
    "\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "y_pred_lbfgs_withoutnltk = CRF_model_lbfgs.predict(X_test)\n",
    "print(\"CRF_model_lbfgs (word level) : \", round(metrics.flat_accuracy_score(y_test, y_pred_lbfgs_withoutnltk)*100, 4))\n",
    "print(\"CRF_model_lbfgs (sentence level) : \", round(metrics.sequence_accuracy_score(y_test, y_pred_lbfgs_withoutnltk)*100, 4))\n",
    "print(\"CRF_model_lbfgs    : \\n\", metrics.flat_classification_report(y_test, y_pred_lbfgs_withoutnltk), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8cc0887c-086f-4d9b-89c5-315358701d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'ADV', 'VERB', 'PUNCT', 'AUX', 'NOUN', 'ADJ', 'ADP', 'CCONJ', 'X', 'PROPN', 'DET', 'PART', 'INTJ', 'NUM', 'SCONJ', 'SYM']\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.9/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=CRF(algorithm='lbfgs',\n",
       "                                 all_possible_transitions=True, c1=0.25,\n",
       "                                 c2=0.35, keep_tempfiles=None,\n",
       "                                 max_iterations=100),\n",
       "                   n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={'c1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fa5823e7520>,\n",
       "                                        'c2': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fa5874a99d0>},\n",
       "                   scoring=make_scorer(flat_f1_score, average=weighted, labels=['PRON', 'ADV', 'VERB', 'PUNCT', 'AUX', 'NOUN', 'ADJ', 'ADP', 'CCONJ', 'X', 'PROPN', 'DET', 'PART', 'INTJ', 'NUM', 'SCONJ', 'SYM']),\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Search model \n",
    "\n",
    "labels = list(CRF_model_lbfgs.classes_)\n",
    "# labels.remove('O')\n",
    "print(labels)\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs_crf_lbfgs = RandomizedSearchCV(CRF_model_lbfgs, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs_crf_lbfgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a4f81-0166-4442-ac7d-a4795b91555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('random search best params:', rs.best_params_)\n",
    "print('random search best CV score:', rs.best_score_)\n",
    "print('random search model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n",
    "\n",
    "\n",
    "y_pred = rs.predict(X_test)\n",
    "\n",
    "print(\"random search CRF_model_lbfgs : \", round(metrics.flat_accuracy_score(y_test, y_pred)*100, 4))\n",
    "print(\" random searchCRF_model_lbfgs : \", round(metrics.sentence_accuracy_score(y_test, y_pred)*100, 4))\n",
    "print(\"random search CRF_model_lbfgs    : \\n\", )\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cb8c60-25c8-41f2-a60c-439290a1bb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melex-kernel",
   "language": "python",
   "name": "melex-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
