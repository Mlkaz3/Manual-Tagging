{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b62409-9c59-4b47-9bea-ab0c04fa0488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time\n",
    "\n",
    "import sklearn\n",
    "import sklearn_crfsuite\n",
    "import scipy.stats\n",
    "import math, string, re\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn_crfsuite import CRF\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag.util import untag\n",
    "\n",
    "import ast\n",
    "from ast import literal_eval\n",
    "\n",
    "import jieba \n",
    "from hanziconv import HanziConv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2527469e-e780-4db1-a10c-9ba837969de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6318 entries, 0 to 6317\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   tagged  6318 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 49.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# read in the seniors previous training data \n",
    "df = pd.read_csv(\"seniors_data.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7420aa6c-ac00-46ba-b78c-9dd18f06c357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[('In', 'ADP'), ('a', 'DET'), ('webinar', 'NOU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[('I', 'PRON'), ('think', 'VERB'), ('that', 'D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[('Goooood', 'X'), (',', 'PUNCT'), ('very', 'A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[('!', 'PUNCT'), ('nice', 'ADJ'), ('to', 'ADP'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[('item', 'NOUN'), ('with', 'ADP'), ('nice', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tagged\n",
       "0  [('In', 'ADP'), ('a', 'DET'), ('webinar', 'NOU...\n",
       "1  [('I', 'PRON'), ('think', 'VERB'), ('that', 'D...\n",
       "2  [('Goooood', 'X'), (',', 'PUNCT'), ('very', 'A...\n",
       "3  [('!', 'PUNCT'), ('nice', 'ADJ'), ('to', 'ADP'...\n",
       "4  [('item', 'NOUN'), ('with', 'ADP'), ('nice', '..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "afc400d6-dd17-49b2-8e5c-a27c545a0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string2_list(text):\n",
    "    return ast.literal_eval(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9e34c0e4-7cfe-4aad-b502-add4dc7b4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "    # \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    \n",
    "    feature_set =  {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'prefix-4': sentence[index][:4],\n",
    "        'prefix-5': sentence[index][:5],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'suffix-4': sentence[index][-4:],\n",
    "        'suffix-5': sentence[index][-5:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'prev2_word': '' if index == 0 else sentence[index - 2],\n",
    "        'next2_word': '' if index == len(sentence) - 2 or index == len(sentence) - 1  else sentence[index + 2],\n",
    "        'prev3_word': '' if index == 0 else sentence[index - 3],\n",
    "        'next3_word': '' if index == len(sentence) - 2 or index == len(sentence) - 1  or index == len(sentence) - 3  else sentence[index + 3],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "        'natural_number': (re.findall(r'^[0-9]+', sentence[index])),\n",
    "        'initcaps' : (re.findall(r'^[A-Z]\\w+', sentence[index])),\n",
    "        'initcapsalpha': (re.findall(r'^[A-Z][a-z]\\w+', sentence[index])),\n",
    "        'word.stemmed': re.sub(r'(.{2,}?)([aeiougyn]+$)',r'\\1', sentence[index].lower()),\n",
    "        'word.ispunctuation': (sentence[index] in string.punctuation)\n",
    "    }\n",
    "    \n",
    "    if index <= 0:\n",
    "        feature_set['BOS'] = True\n",
    "    \n",
    "    if index > len(sentence)-1:\n",
    "        feature_set['EOS'] = True\n",
    "        \n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5cca0b04-e0f3-4f7d-a907-aa70d1f1028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        X.append([features(untag(tagged), index) for index in range(len(tagged))])\n",
    "        y.append([tag for _, tag in tagged])\n",
    " \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6485d896-e9a4-4e93-a034-ce725d9633be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(sentence, model):\n",
    "    sentence = sentence_splitter(sentence)\n",
    "    sentence_features = [features(sentence, index) for index in range(len(sentence))]\n",
    "    return list(zip(sentence, model.predict([sentence_features])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4af68e1d-5568-4c25-847b-707602cd1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_splitter(sentence):\n",
    "    result = []\n",
    "    sents = word_tokenize(sentence)\n",
    "    for s in sents:\n",
    "        if re.findall(r'[\\u4e00-\\u9fff]+', s):\n",
    "            s = HanziConv.toSimplified(s)\n",
    "            result = result + list(jieba.cut(s, cut_all=False))\n",
    "        else:\n",
    "            result.append(s)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cfb4b7ca-0dab-4f1b-881c-6b48d93f2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset for training and testing\n",
    "data = df.tagged.apply(convert_string2_list)\n",
    "data = data.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7181a641-c2ec-4273-8f1d-b933c71d7f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(.80 * len(data))\n",
    "training_sentences = data[:cutoff]\n",
    "test_sentences = data[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7ddee64c-f2af-417d-a9ea-521cf764870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = transform_to_dataset(training_sentences)\n",
    "X_test, y_test = transform_to_dataset(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7b45394a-4e62-4591-929f-90a362a2cad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5054\n",
      "1264\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))     \n",
    "print(len(X_test))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bdac6a4d-849e-4f58-abdc-be2c7211e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRF_model_lbfgs = sklearn_crfsuite.CRF(\n",
    "    algorithm = 'lbfgs',\n",
    "    max_iterations = 100,\n",
    "    all_possible_transitions=True,\n",
    "    c1 = 0.25,\n",
    "    c2 = 0.35\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "095ff09c-4569-407a-a0a8-de0bce913ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc8/anaconda3/envs/MELex/lib/python3.8/site-packages/sklearn/base.py:193: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None, all_possible_transitions=True,\n",
       "    averaging=None, c=None, c1=0.25, c2=0.35, calibration_candidates=None,\n",
       "    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
       "    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "    max_linesearch=None, min_freq=None, model_filename=None, num_memories=None,\n",
       "    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRF_model_lbfgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e801fb38-16be-4ebb-bc10-7240a97f8862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5054"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e6c9f2ce-f550-47d3-947a-d4cb866ee525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on the test set = 0.969186531240904\n",
      "\n",
      "Accuracy on the test set = 0.9691030739127691\n",
      "\n",
      "Precision on the test set = 0.969311564152366\n",
      "\n",
      "Recall on the test set = 0.9691030739127691\n",
      "\n",
      "Test set classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           X      0.998     1.000     0.999      2127\n",
      "        PART      0.997     1.000     0.998       991\n",
      "       CCONJ      0.999     0.994     0.996       683\n",
      "       SCONJ      0.000     0.000     0.000         0\n",
      "         ADJ      0.868     0.898     0.883      1988\n",
      "         ADP      0.980     0.981     0.981      3153\n",
      "         ADV      0.881     0.871     0.876       793\n",
      "        VERB      0.968     0.962     0.965      4206\n",
      "         DET      0.992     0.995     0.993      2837\n",
      "        INTJ      0.000     0.000     0.000         0\n",
      "        NOUN      0.954     0.950     0.952      6313\n",
      "        PRON      0.996     1.000     0.998       479\n",
      "       PROPN      0.969     0.962     0.965      2918\n",
      "         NUM      0.999     0.994     0.997      1599\n",
      "       PUNCT      1.000     1.000     1.000      3273\n",
      "         AUX      0.000     0.000     0.000         0\n",
      "         SYM      0.997     1.000     0.998       326\n",
      "\n",
      "   micro avg      0.969     0.969     0.969     31686\n",
      "   macro avg      0.800     0.800     0.800     31686\n",
      "weighted avg      0.969     0.969     0.969     31686\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc8/anaconda3/envs/MELex/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = CRF_model_lbfgs.predict(X_test)\n",
    "print('F1 score on the test set = {}\\n'.format(metrics.flat_f1_score(y_test, y_pred, labels=labels, average='weighted')))\n",
    "print('Accuracy on the test set = {}\\n'.format(metrics.flat_accuracy_score(y_test, y_pred)))\n",
    "print('Precision on the test set = {}\\n'.format(metrics.flat_precision_score(y_test, y_pred, labels=labels, average='weighted')))\n",
    "print('Recall on the test set = {}\\n'.format(metrics.flat_recall_score(y_test, y_pred,labels=labels, average='weighted')))\n",
    "\n",
    "\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print('Test set classification report: \\n\\n{}'.format(metrics.flat_classification_report(\n",
    "y_test, y_pred, labels=sorted_labels, digits=3\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4578a46a-6105-443a-8a8a-6f15b89d566e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT', 'PROPN', 'PART', 'VERB', 'ADV', 'CCONJ', 'AUX', 'PRON', 'X', 'INTJ', 'SCONJ', 'NUM', 'SYM']\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:  6.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "                                 all_possible_transitions=True, averaging=None,\n",
       "                                 c=None, c1=0.25, c2=0.35,\n",
       "                                 calibration_candidates=None,\n",
       "                                 calibration_eta=None,\n",
       "                                 calibration_max_trials=None,\n",
       "                                 calibration_rate=None,\n",
       "                                 calibration_samples=None, delta=None,\n",
       "                                 epsilon=None, error_sensitive=None, gamma=None,\n",
       "                                 keep_...\n",
       "                   param_distributions={'c1': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7f783b3bf1c0>,\n",
       "                                        'c2': <scipy.stats._distn_infrastructure.rv_continuous_frozen object at 0x7f783708fd00>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False,\n",
       "                   scoring=make_scorer(flat_f1_score, average=weighted, labels=['ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT', 'PROPN', 'PART', 'VERB', 'ADV', 'CCONJ', 'AUX', 'PRON', 'X', 'INTJ', 'SCONJ', 'NUM', 'SYM']),\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(CRF_model_lbfgs.classes_)\n",
    "# labels.remove('O')\n",
    "print(labels)\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(CRF_model_lbfgs, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4fa9ea71-07fb-4c81-a184-00fbb33c8582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "                           all_possible_transitions=True, averaging=None,\n",
       "                           c=None, c1=0.25, c2=0.35,\n",
       "                           calibration_candidates=None, calibration_eta=None,\n",
       "                           calibration_max_trials=None, calibration_rate=None,\n",
       "                           calibration_samples=None, delta=None, epsilon=None,\n",
       "                           error_sensitive=None, gamma=None,\n",
       "                           keep_tempfi...\n",
       "                           trainer_cls=None, variance=None, verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'c1': [0, 0.05, 0.1, 0.25, 0.5, 1],\n",
       "                         'c2': [0, 0.05, 0.1, 0.25, 0.5, 1]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(flat_f1_score, average=weighted, labels=['ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT', 'PROPN', 'PART', 'VERB', 'ADV', 'CCONJ', 'AUX', 'PRON', 'X', 'INTJ', 'SCONJ', 'NUM', 'SYM']),\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params_space = {\n",
    "    \"c1\": [0,0.05,0.1, 0.25,0.5,1],\n",
    "    \"c2\": [0,0.05,0.1, 0.25,0.5,1]\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "grid_search = GridSearchCV(estimator=CRF_model_lbfgs,\n",
    "                           param_grid=params_space,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1, verbose=1,scoring=f1_scorer)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "64bc57a1-7d8f-4e92-82a7-0601d4ca807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1708beb2-3285-4cc0-8533-bcb07df31adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF_model_lbfgs :  96.9008\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           X      0.997     1.000     0.998      2127\n",
      "        PART      0.996     1.000     0.998       991\n",
      "       CCONJ      0.999     0.996     0.997       683\n",
      "       SCONJ      0.000     0.000     0.000         0\n",
      "         ADJ      0.868     0.897     0.883      1988\n",
      "         ADP      0.982     0.982     0.982      3153\n",
      "         ADV      0.866     0.878     0.872       793\n",
      "        VERB      0.967     0.962     0.965      4206\n",
      "         DET      0.991     0.994     0.992      2837\n",
      "        INTJ      0.000     0.000     0.000         0\n",
      "        NOUN      0.955     0.949     0.952      6313\n",
      "        PRON      0.994     1.000     0.997       479\n",
      "       PROPN      0.972     0.961     0.966      2918\n",
      "         NUM      0.999     0.994     0.997      1599\n",
      "       PUNCT      1.000     1.000     1.000      3273\n",
      "         AUX      0.000     0.000     0.000         0\n",
      "         SYM      1.000     1.000     1.000       326\n",
      "\n",
      "   micro avg      0.969     0.969     0.969     31686\n",
      "   macro avg      0.799     0.801     0.800     31686\n",
      "weighted avg      0.969     0.969     0.969     31686\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc8/anaconda3/envs/MELex/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/pc8/anaconda3/envs/MELex/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_search.predict(X_test)\n",
    "print(\"CRF_model_lbfgs : \", round(metrics.flat_accuracy_score(y_test, y_pred)*100, 4))\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "13655113-4adc-4cb5-a026-f76872176e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF_model_lbfgs :  96.9103\n",
      "CRF_model_lbfgs    : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.87      0.90      0.88      1988\n",
      "         ADP       0.98      0.98      0.98      3153\n",
      "         ADV       0.88      0.87      0.88       793\n",
      "         AUX       0.00      0.00      0.00         0\n",
      "       CCONJ       1.00      0.99      1.00       683\n",
      "         DET       0.99      1.00      0.99      2837\n",
      "        NOUN       0.95      0.95      0.95      6313\n",
      "         NUM       1.00      0.99      1.00      1599\n",
      "        PART       1.00      1.00      1.00       991\n",
      "        PRON       1.00      1.00      1.00       479\n",
      "       PROPN       0.97      0.96      0.97      2918\n",
      "       PUNCT       1.00      1.00      1.00      3273\n",
      "         SYM       1.00      1.00      1.00       326\n",
      "        VERB       0.97      0.96      0.97      4206\n",
      "           X       1.00      1.00      1.00      2127\n",
      "\n",
      "    accuracy                           0.97     31686\n",
      "   macro avg       0.91      0.91      0.91     31686\n",
      "weighted avg       0.97      0.97      0.97     31686\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "y_pred_lbfgs = CRF_model_lbfgs.predict(X_test)\n",
    "print(\"CRF_model_lbfgs : \", round(metrics.flat_accuracy_score(y_test, y_pred_lbfgs)*100, 4))\n",
    "print(\"CRF_model_lbfgs    : \\n\", metrics.flat_classification_report(y_test, y_pred_lbfgs), \"\\n\")\n",
    "\n",
    "# y_pred_l2sgd = CRF_model_l2sgd.predict(X_test)\n",
    "# print(\"CRF_model_l2sgd : \", round(metrics.flat_accuracy_score(y_test, y_pred_l2sgd)*100, 4))\n",
    "# print(\"CRF_model_l2sgd    : \\n\", metrics.flat_classification_report(y_test, y_pred_l2sgd), \"\\n\")\n",
    "\n",
    "# y_pred_arow = CRF_model_arow.predict(X_test)\n",
    "# print(\"CRF_model_arow  : \", round(metrics.flat_accuracy_score(y_test, y_pred_arow)*100, 4))\n",
    "# print(\"CRF_model_arow    : \\n\", metrics.flat_classification_report(y_test, y_pred_arow), \"\\n\")\n",
    "\n",
    "# y_pred_pa = CRF_model_pa.predict(X_test)\n",
    "# print(\"CRF_model_pa    : \", round(metrics.flat_accuracy_score(y_test, y_pred_pa)*100, 4))\n",
    "# print(\"CRF_model_pa    : \\n\", metrics.flat_classification_report(y_test, y_pred_pa), \"\\n\")\n",
    "\n",
    "# y_pred_ap = CRF_model_ap.predict(X_test)\n",
    "# print(\"CRF_model_ap    : \", round(metrics.flat_accuracy_score(y_test, y_pred_ap)*100, 4))\n",
    "# print(\"CRF_model_ap    : \\n\", metrics.flat_classification_report(y_test, y_pred_ap), \"\\n\")\n",
    "\n",
    "# print(metrics.sequence_accuracy_score(y_test, y_pred_lbfgs)*100)\n",
    "# https://sklearn-crfsuite.readthedocs.io/en/latest/_modules/sklearn_crfsuite/metrics.html\n",
    "# CRF_model_lbfgs :  97.664\n",
    "# CRF_model_l2sgd :  97.3046\n",
    "# CRF_model_arow  :  99.7604\n",
    "# CRF_model_pa    :  99.7904\n",
    "# CRF_model_ap    :  99.8203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7f8dbc-ada9-4feb-9bd9-e163d6a41601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melex-kernel",
   "language": "python",
   "name": "melex-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
